{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEL DATA EXPLORATION OLD\n",
    "This Jupiter notebook is used to explore the data and record my thought process of how I will approach the problem. The first step is to import the necessary libraries, and then load the data. This notebook uses the data from multiple seperate sources meaning the results aren't as good as the new notebook. However,this notebook is still useful as it shows the thought process behind the data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "Currently we are using data from the \"Carradale Water at Dippen\" Water Station. This is a water station which is located in Cambeltown, on the West Coast of Scotland. The reason for choosing this station is because it has approximately the same latitude, and geography as the fictitious island of Claddach and monitors a river pretty much all the way from the source to the river estuary. This means that the data should be representative of the data we would expect to see on Claddach.\n",
    "\n",
    "The data comes from 4 seperate CSV files which are accessed from the following sources:\n",
    "- National River Flow Archive (https://nrfa.ceh.ac.uk/data/station/spatial/88001); This is used to get the gauged daily flow and the catchment daily rainfall data.\n",
    "- River Levels UK (https://riverlevels.uk/carradale-water-dippen#.Y8e3-9LP1hE); This is used to get the daily river level data.\n",
    "- Scottish Environmental Protection Agency (https://www2.sepa.org.uk/waterlevels/?sd=t&lc=133077); This is used to get recent level data in more detail (quater-hourly opposed to daily, however only for the last 5 days.)\n",
    "\n",
    "If we decide in the future to model our data using a different water station, we need to ensure that it records all 4 of the CSV files which are mentioned above. Before reading in the data, we took the following steps to format the data so that it is easier to read in and work with:\n",
    "- Downloaded, renamed, and moved the CSV files to the data folder.\n",
    "- Removed the first few rows of the CSV files which contained information about the data, and not the data itself.\n",
    "\n",
    "Improvements which could be made in the future:\n",
    "- Instead of using just one water station, we could aggregate data from multiple water stations to get a more representative sample of the data.\n",
    "- Find quater-hourly data for the rainfall and flow data, as this could then be used with the SEPA data to get a more short-term view of the data.\n",
    "- Find data for extreme weather events, such as floods, to see how the data changes during these events.\n",
    "- Currently the data time frames have very little overlap (only 2017) which means despite having some data dating back to 1960s we can only use a small portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26157/4235383111.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  flow = pd.read_csv('Data/NATIONAL_RIVER_FLOW_ARCHIVE_GAUGED_DAILY_FLOW.csv', error_bad_lines=False) # When reading the csv file, ignore the bad lines\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/NATIONAL_RIVER_FLOW_ARCHIVE_GAUGED_DAILY_FLOW.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use numpy to read the 4 csv files into 4 seperate numpy arrays\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m flow \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mData/NATIONAL_RIVER_FLOW_ARCHIVE_GAUGED_DAILY_FLOW.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, error_bad_lines\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# When reading the csv file, ignore the bad lines\u001b[39;00m\n\u001b[1;32m      3\u001b[0m rain \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mData/NATIONAL_RIVER_FLOW_ARCHIVE_CATCHMENT_DAILY_RAINFALL.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m level \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mData/RIVER_LEVELS_UK.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/NATIONAL_RIVER_FLOW_ARCHIVE_GAUGED_DAILY_FLOW.csv'"
     ]
    }
   ],
   "source": [
    "# Use numpy to read the 4 csv files into 4 seperate numpy arrays\n",
    "flow = pd.read_csv('Data/NATIONAL_RIVER_FLOW_ARCHIVE_GAUGED_DAILY_FLOW.csv', error_bad_lines=False) # When reading the csv file, ignore the bad lines\n",
    "rain = pd.read_csv('Data/NATIONAL_RIVER_FLOW_ARCHIVE_CATCHMENT_DAILY_RAINFALL.csv')\n",
    "level = pd.read_csv('Data/RIVER_LEVELS_UK.csv')\n",
    "level_recent = pd.read_csv('Data/SEPA_LEVEL_DATA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 3 datasets into one - excluding the level_recent dataset as it uses dataTime opposed to just date.\n",
    "merged = pd.merge(flow, rain, on=['date'])\n",
    "merged = pd.merge(merged, level, on=['date'])\n",
    "merged.dropna(inplace=True)\n",
    "merged = merged.drop('ignore', axis=1)\n",
    "merged.describe()\n",
    "\n",
    "# Find the number of days with more than 1mm of rain\n",
    "merged[merged['cdr(mm)'] > 1].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "First stage of exploring the data is to create a basic plot to see what the interactions are between the flow, rainfall and river level data. After this, we can start to look at the data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(merged['date'], [i * 50 for i in merged['avg_level']])\n",
    "#plt.plot(merged['date'], merged['gdf(m3/s)'])\n",
    "plt.plot(merged['date'], merged['cdr(mm)'])\n",
    "plt.legend(['Level', 'Rainfall'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by the average rainfall (independent variable) and plot the average flow and level (dependent variables)\n",
    "byRainfall = merged.sort_values(by=['cdr(mm)'])\n",
    "plt.plot(byRainfall['cdr(mm)'], byRainfall['gdf(m3/s)'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(byRainfall['cdr(mm)'], byRainfall['avg_level'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to plot average level against average flow\n",
    "plt.plot(byRainfall['gdf(m3/s)'], byRainfall['avg_level'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 3 variables on a 3D graph\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(byRainfall['gdf(m3/s)'], byRainfall['avg_level'], byRainfall['cdr(mm)'], c=byRainfall['cdr(mm)'], cmap='Greens');\n",
    "ax.set_xlabel('Flow')\n",
    "ax.set_ylabel('Level')\n",
    "ax.set_zlabel('Rainfall')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Calculations\n",
    "\n",
    "- The Catchment Area of the Carradale Water at Dippen Water Station is 58.5km^2 (https://www2.sepa.org.uk/waterlevels/default.aspx?sd=t&lc=133077).\n",
    "- 1mm of rainfall is equal to 1 litre of water per square metre.\n",
    "- hence the amount of water going into our river (measured in litres) on a given day is equal to: rainfall * 58.5 * 1,000,000 (to convert from km^2 to m^2)\n",
    "- = rainfall * 58,500,000\n",
    "\n",
    "- For the amount of water going out of the river (measured in litres) on a given day, we can use the flow data.\n",
    "- We know that the flow data is measured in m^3/s, so we can convert this to litres (by multiplying by 1000) and then multiply by the number of seconds in a day (86400) to get the litres of water going out of the river on a given day.\n",
    "- = flow * 1000 * 86400\n",
    "- = flow * 86,400,000\n",
    "\n",
    "Hence the water difference on a given day is equal to:\n",
    "- = rainfall * 58,500,000 - flow * 86,400,000\n",
    "\n",
    "Let's add this to our dataframe and plot it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterDifference = [merged['cdr(mm)'][i]*58500000 - merged['gdf(m3/s)'][i]*86400000 for i in range(len(merged['cdr(mm)']))]\n",
    "cumulativeWaterDifference = [sum(waterDifference[:i]) for i in range(len(waterDifference))]\n",
    "\n",
    "print(\"minimum cumulative water difference:\", min(cumulativeWaterDifference))\n",
    "print(\"maximum cumulative water difference:\",max(cumulativeWaterDifference))\n",
    "print(\"mean cumulative water difference:\",sum(cumulativeWaterDifference)/len(cumulativeWaterDifference))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above numbers, we seem to be gaining a lot more water than we are losing. This is likely because we made two assumptions when calculating the water difference:\n",
    "- We assumed that all of the rainfall was eventually going into the river.\n",
    "- We didn't take avaperation into account.\n",
    "\n",
    "The Scottish Governments states that the estimated percentage of precipitation which runoffs into the see is approximately 73% (https://www.gov.scot/publications/scotlands-marine-atlas-information-national-marine-plan/pages/7/) However, this is extremely variable from location to location (eg. in the USA this is as low as 30% (https://www.usgs.gov/special-topics/water-science-school/science/rain-and-precipitation). As well as the fact that on a daily, or even hourly basis, this will depend upon the weather conditions as more sunshine and warmer weather will lead to more evaporation and less ending up in the river. Therefore for now, we will set the percentage of rainfall which ends up in the river to a figure which results in the cumulative water difference throughout the year being equal to 0. This will likely be changed later to a more realistic value which changes throughout the year to account for the weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the percentage that makes the culmulative water difference the closest to 0 we will use a binary search algorithm, starting with a percentage of 0.5.\n",
    "percentage = 0.5\n",
    "scale = 0.5\n",
    "waterDifference = [merged['cdr(mm)'][i]*58500000*percentage - merged['gdf(m3/s)'][i]*86400000 for i in range(len(merged['cdr(mm)']))]\n",
    "cumulativeWaterDifference = [sum(waterDifference[:i]) for i in range(len(waterDifference))]\n",
    "averageCum = sum(cumulativeWaterDifference)/len(cumulativeWaterDifference)\n",
    "\n",
    "while (averageCum >= 100 or averageCum <= -100):\n",
    "    waterDifference = [merged['cdr(mm)'][i]*58500000*percentage - merged['gdf(m3/s)'][i]*86400000 for i in range(len(merged['cdr(mm)']))]\n",
    "    cumulativeWaterDifference = [sum(waterDifference[:i]) for i in range(len(waterDifference))]\n",
    "    averageCum = sum(cumulativeWaterDifference)/len(cumulativeWaterDifference)\n",
    "    if averageCum >= 100:\n",
    "        percentage -= scale\n",
    "    elif averageCum <= -100:\n",
    "        percentage += scale\n",
    "    scale /= 2\n",
    "\n",
    "\n",
    "print(\"Calculated percentage of water which ends up in river\", percentage)\n",
    "\n",
    "waterDifference = [merged['cdr(mm)'][i]*58500000*percentage - merged['gdf(m3/s)'][i]*86400000 for i in range(len(merged['cdr(mm)']))]\n",
    "cumulativeWaterDifference = [sum(waterDifference[:i]) for i in range(len(waterDifference))]\n",
    "\n",
    "print(\"\\nminimum cumulative water difference:\", min(cumulativeWaterDifference))\n",
    "print(\"maximum cumulative water difference:\",max(cumulativeWaterDifference))\n",
    "print(\"mean cumulative water difference:\",sum(cumulativeWaterDifference)/len(cumulativeWaterDifference))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final calculated percentage of water which ends up in the river is 64.7%, which is fairly close to the Scotish Governments stated estimate of 73%. In the future we could look at using a more accurate value for this, or even a value which changes throughout the year to account for the weather conditions and explore why their is a difference between the two values. However, for now we will continue with the assumption that 64.7% of the rainfall ends up in the river and continue with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterDifference = [merged['cdr(mm)'][i]*58500000*0.697 - merged['gdf(m3/s)'][i]*86400000 for i in range(len(merged['cdr(mm)']))]\n",
    "cumulativeWaterDifference = [sum(waterDifference[:i]) for i in range(len(waterDifference))]\n",
    "\n",
    "merged['water_difference'] = waterDifference\n",
    "merged['cumulative_water_difference'] = cumulativeWaterDifference\n",
    "merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the water difference and cumulative water difference\n",
    "plt.plot(merged['date'], merged['water_difference'])\n",
    "plt.plot(merged['date'], merged['cumulative_water_difference'])\n",
    "plt.legend(['Water Difference', 'Cumulative Water Difference'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the water difference and the level against the date\n",
    "plt.plot(merged['date'], merged['water_difference'] / 1000000000)\n",
    "plt.plot(merged['date'], merged['avg_level'])\n",
    "plt.legend(['Water Difference (in billions)', 'Level'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the graph above, when we have a day with a large positive water difference, the water level rises shortly after. Likewise, when we have a day of negative rainfall, we see the water difference decrease as we'd expect. The next step will be to plot the differentiated water level as this will give us a better idea of the change in water level dependent upon the water difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of the water level\n",
    "levelDerivative = [merged['avg_level'][i+1] - merged['avg_level'][i] for i in range(len(merged['avg_level'])-1)]\n",
    "levelDerivative.append(0)\n",
    "merged['level_derivative'] = levelDerivative\n",
    "\n",
    "# Plot the water difference and the level derivative against the date with different axis scales\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(merged['date'], merged['water_difference'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Water Difference', color='tab:blue')\n",
    "ax1.set_ylim(-1000000000, 1000000000)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged['date'], merged['level_derivative'], color='tab:red')\n",
    "ax2.set_ylabel('Level Derivative', color='tab:red')\n",
    "ax2.set_ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn that looks good :) love it when the data looks like it's doing what you expect it to do. The reason why the water difference is divided by a billion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the level_derivative against the rainfall\n",
    "merged.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['gdf(m3/s)', 'cdr(mm)', 'avg_level', 'water_difference', 'level_derivative']\n",
    "sns.pairplot(merged[cols], height=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_std = stdsc.fit_transform(merged[cols].values)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "plt.figure(figsize=(10, 10))\n",
    "hm = sns.heatmap(cov_mat, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 15}, cmap='coolwarm', yticklabels=cols, xticklabels=cols)\n",
    "plt.title('Covariance Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
